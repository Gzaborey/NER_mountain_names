The solution can be divided into two main stages:

1. Dataset creation
2. Model training

It was not possible to find a ready-made dataset that would fit the task. So, it was decided to create our own dataset using synthetic data obtained from ChatGPT.

First, a small corpus of texts about mountains was collected, which was generated by chatGPT.

Next, through experiments, a prompt was derived, which made it possible to obtain a tokenized and labeled version of the sentences. The main problem that existed initially was that ChatGPT created more labels than there were tokens, which would make creating a dataset impossible. The solution was found to force ChatGPT to represent the result as a pair (token, label). In this version, the model was no longer mistaken in its calculations.

Next, the results, presented in the form of strings, were processed using regular expressions and eventually placed in the dataset.

One of the best models in the classification of tokens is BERT, so the choice fell on it. As it was pre-trained, some finetuning was done to match the 'mountains_names.csv' dataset.

Steps to set up a project:
1. Install dependencies.
2. Run the 'train.py' script. The resulting model will be saved in the 'outputs' folder.
3. To use a trained model, run 'inference.py'. Change the value of the 'model_path' variable, to the destination of the 'outputs' folder. Change the value of the 'text' variable, to analyze different texts.
